{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "press any one of the valid keys\n",
      "colour code\n",
      "Speak Please\n"
     ]
    },
    {
     "ename": "UnknownValueError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnknownValueError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-232edcffed7c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    389\u001b[0m         \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    390\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mkeyboard\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_pressed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'e'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# if key 'e' is pressed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 391\u001b[1;33m         \u001b[0msign_to_speech\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    392\u001b[0m         \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    393\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mkeyboard\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_pressed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m't'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# if key 't' is pressed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-23-232edcffed7c>\u001b[0m in \u001b[0;36msign_to_speech\u001b[1;34m()\u001b[0m\n\u001b[0;32m    169\u001b[0m             \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mkeyboard\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_pressed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# if key 'c' is pressed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 171\u001b[1;33m             \u001b[0mcolour_recognition\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    172\u001b[0m             \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mkeyboard\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_pressed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-23-232edcffed7c>\u001b[0m in \u001b[0;36mcolour_recognition\u001b[1;34m()\u001b[0m\n\u001b[0;32m    366\u001b[0m             \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    367\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mkeyboard\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_pressed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'w'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# if key 'w' is pressed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 368\u001b[1;33m             \u001b[0mspeech_to_sign\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    369\u001b[0m             \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    370\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mkeyboard\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_pressed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'e'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# if key 'e' is pressed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-23-232edcffed7c>\u001b[0m in \u001b[0;36mspeech_to_sign\u001b[1;34m()\u001b[0m\n\u001b[0;32m    222\u001b[0m         \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madjust_for_ambient_noise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maudio_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[0maudio\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlisten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maudio_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 224\u001b[1;33m         \u001b[0mt\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecognize_google\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maudio\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    225\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Converting Speech to Text...\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"You said: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\speech_recognition\\__init__.py\u001b[0m in \u001b[0;36mrecognize_google\u001b[1;34m(self, audio_data, key, language, show_all)\u001b[0m\n\u001b[0;32m    856\u001b[0m         \u001b[1;31m# return results\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    857\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mshow_all\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mreturn\u001b[0m \u001b[0mactual_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 858\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactual_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactual_result\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"alternative\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mraise\u001b[0m \u001b[0mUnknownValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    859\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    860\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;34m\"confidence\"\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mactual_result\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"alternative\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnknownValueError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import keras\n",
    "import os\n",
    "from threading import Thread\n",
    "from gtts import gTTS \n",
    "from playsound import playsound \n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import tensorflow as tf\n",
    "import pyttsx3\n",
    "import mediapipe as mp\n",
    "import time\n",
    "import os\n",
    "from gtts import gTTS \n",
    "from playsound import playsound \n",
    "import cv2\n",
    "import re\n",
    "import time\n",
    "from moviepy.editor import *\n",
    "import keyboard\n",
    "import speech_recognition as sr\n",
    "import time\n",
    "from color_recognition import histogram\n",
    "from color_recognition import knn\n",
    "\n",
    "\n",
    "print(\"press any one of the valid keys\")\n",
    "\n",
    "background= None\n",
    "def say_text(text):\n",
    "    engine = pyttsx3.init()\n",
    "    while engine._inLoop:\n",
    "        pass\n",
    "    engine.setProperty(\"rate\", 105)\n",
    "    engine.say(text)\n",
    "    engine.runAndWait()\n",
    "\n",
    "def cal_accum_avg(frame, accumulated_weight):\n",
    "    global background\n",
    "    if background is None:\n",
    "        background = frame.copy().astype(\"float\")\n",
    "        return None\n",
    "    cv2.accumulateWeighted(frame, background, accumulated_weight)\n",
    "\n",
    "def segment_hand(frame, threshold=25):\n",
    "    global background\n",
    "    global contour\n",
    "    diff = cv2.absdiff(background.astype(\"uint8\"), frame)\n",
    "    _ , thresholded = cv2.threshold(diff, threshold, 255, cv2.THRESH_BINARY)    \n",
    "    #Fetching contours in the frame (These contours can be of hand or any other object in foreground) ...\n",
    "    contours, hierarchy = cv2.findContours(thresholded.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    # If length of contours list = 0, means we didn't get any contours...\n",
    "    if len(contours) == 0:\n",
    "        return None\n",
    "    else:\n",
    "        # The largest external contour should be the hand \n",
    "        hand_segment_max_cont = max(contours, key=cv2.contourArea)  \n",
    "        contour = max(contours, key = cv2.contourArea)\n",
    "        # Returning the hand segment(max contour) and the thresholded image of hand...\n",
    "        return (thresholded, hand_segment_max_cont)\n",
    "\n",
    "def sign_to_speech():\n",
    "    model = keras.models.load_model(\"best_model.h5\")\n",
    "    mp_drawing = mp.solutions.drawing_utils\n",
    "    mp_hands = mp.solutions.hands\n",
    "    data=\"\"\n",
    "    text=\"\"\n",
    "    count_same_frame = 0\n",
    "    count_same_frame1 = 0\n",
    "    count_same_frame2 = 0\n",
    "    background = None\n",
    "    accumulated_weight = 0.5\n",
    "    ROI_top = 100\n",
    "    ROI_bottom = 300\n",
    "    ROI_right = 150\n",
    "    ROI_left = 350\n",
    "    word_dict = {0:'0', 1:'1', 2:'10', 3:'2', 4:'3', 5:'4', 6:'5', 7:'6', 8:'7', 9:'8', 10:'9', \n",
    "             11:'Enemy has rifle', 12:'Enemy sniper in distance', 13:'Freeze', 14:'I am fully equipped',\n",
    "             15:'Stop', 16:'Understood', 17:'air support inbound', 18:'cease fire',19:'friends in captivity'\n",
    "             ,20:'hostages at target location', 21:'houses nearby', 22:'need ammunition', 23:'no', 24:'radio connection lost', \n",
    "             25:'salute', 26:'search the perimeter', 27:'waiting for confirmation', 28:'wrong salute(friend in danger)', 29:'yes'}\n",
    "    \n",
    "    cam=cv2.VideoCapture(0)\n",
    "    num_frames =0\n",
    "    with mp_hands.Hands(min_detection_confidence=0.5,min_tracking_confidence=0.5) as hands:\n",
    "        while True:\n",
    "            ret, frame = cam.read()\n",
    "            frame = cv2.flip(frame, 1)\n",
    "            image1 = frame[ROI_top:ROI_bottom, ROI_right:ROI_left]\n",
    "        #image1 = cv2.flip(image1, 1)\n",
    "            image1 = cv2.cvtColor(image1, cv2.COLOR_BGR2RGB)\n",
    "            image1.flags.writeable = False\n",
    "            results = hands.process(image1)\n",
    "            image1.flags.writeable = True\n",
    "            image1 = cv2.cvtColor(image1, cv2.COLOR_RGB2BGR)\n",
    "            if results.multi_hand_landmarks:\n",
    "                for hand_landmarks in results.multi_hand_landmarks:\n",
    "                    mp_drawing.draw_landmarks(image1, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "            cv2.imshow('MediaPipe Hands', image1)\n",
    "            frame_copy = frame.copy()\n",
    "            roi = frame[ROI_top:ROI_bottom, ROI_right:ROI_left]\n",
    "            gray_frame = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n",
    "            gray_frame = cv2.GaussianBlur(gray_frame, (9, 9), 0)\n",
    "            if num_frames < 151:\n",
    "                cal_accum_avg(gray_frame, accumulated_weight)\n",
    "                cv2.putText(frame_copy, str(num_frames)+\"For\", (70, 45), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)\n",
    "                cv2.putText(frame_copy, \"Saving Background. Please wait for 150 frames\", (80, 350), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,255,255), 2)\n",
    "                cv2.putText(frame_copy, \"Keep background clean for better prediction\", (80, 400), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,255,255), 2)\n",
    "         \n",
    "            else: \n",
    "                hand = segment_hand(gray_frame)\n",
    "                if hand is not None:\n",
    "                    thresholded, hand_segment = hand\n",
    "                    cv2.drawContours(frame_copy, [hand_segment + (ROI_right, ROI_top)], -1, (255, 0, 0),1)\n",
    "                    cv2.imshow(\"Thesholded Hand Image\", thresholded)\n",
    "                    thresholded = cv2.resize(thresholded, (64, 64))\n",
    "                    thresholded = cv2.cvtColor(thresholded, cv2.COLOR_GRAY2RGB)\n",
    "                    thresholded = np.reshape(thresholded, (1,thresholded.shape[0],thresholded.shape[1],3))\n",
    "                    pred = model.predict(thresholded)\n",
    "                    cv2.putText(frame_copy, word_dict[np.argmax(pred)], (170, 45), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)\n",
    "                    old_text=text\n",
    "                    cv2.putText(frame_copy, str(count_same_frame), (30, 20), cv2.FONT_ITALIC, 0.5, (255,0,0), 1)\n",
    "                    if cv2.contourArea(contour) > 4000:\n",
    "                        text = word_dict[np.argmax(pred)]\n",
    "                        if old_text == text:\n",
    "                            count_same_frame += 1\n",
    "                        if old_text!=text:\n",
    "                            count_same_frame=0\n",
    "                        if count_same_frame == 20:\n",
    "                            data = data + text\n",
    "                            data+=\".\"\n",
    "                            data+=\"\\n\"\n",
    "                            count_same_frame =0\n",
    "                else:\n",
    "                    if data != '':\n",
    "                        Thread(target=say_text, args=(data, )).start()\n",
    "                    text = \"\"\n",
    "                    data = \"\"\n",
    "            blackboard = np.zeros((480, 640, 3), dtype=np.uint8)\n",
    "            cv2.putText(blackboard, \" \", (180, 50), cv2.FONT_HERSHEY_TRIPLEX, 1.5, (255, 0,0))\n",
    "            cv2.putText(blackboard, \"Predicted text- \" + text, (30, 100), cv2.FONT_HERSHEY_TRIPLEX, 0.75, (255, 255, 0))\n",
    "            y0, dy = 140, 15\n",
    "            for i, data1 in enumerate(data.split('\\n')):\n",
    "                y = y0+i*dy\n",
    "                cv2.putText(blackboard, data1, (30, y), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255))\n",
    "            cv2.rectangle(frame_copy, (ROI_left, ROI_top), (ROI_right, ROI_bottom), (255,128,0), 3)\n",
    "            num_frames += 1\n",
    "            cv2.putText(frame_copy, str(count_same_frame), (30, 20), cv2.FONT_ITALIC, 0.75, (255,255,255), 1)\n",
    "            res = np.hstack((frame_copy, blackboard))\n",
    "            cv2.imshow(\"Recognizing gesture\", res)\n",
    "            k = cv2.waitKey(1) & 0xFF\n",
    "            if k == 27:\n",
    "                break\n",
    "\n",
    "        cam.release()\n",
    "        cv2.destroyAllWindows()\n",
    "    while True:\n",
    "        if keyboard.is_pressed('q'):  # if key 'q' is pressed \n",
    "            text_to_sign()\n",
    "            break\n",
    "        if keyboard.is_pressed('w'):  # if key 'w' is pressed \n",
    "            speech_to_sign()\n",
    "            break\n",
    "        if keyboard.is_pressed('e'):  # if key 'e' is pressed \n",
    "            sign_to_speech()\n",
    "            break\n",
    "        if keyboard.is_pressed('t'):  # if key 't' is pressed \n",
    "            text_to_speech()\n",
    "            break\n",
    "        if keyboard.is_pressed('c'):  # if key 'c' is pressed \n",
    "            colour_recognition()\n",
    "            break\n",
    "        if keyboard.is_pressed('r'):\n",
    "            print(\"program terminated\")# if key 'r' is pressed \n",
    "            break\n",
    "\n",
    "def text_to_sign():\n",
    "    t=input(\"Enter your message: \")\n",
    "    img=re.split('\\s+', t)\n",
    "    for k in range(len(img)):\n",
    "        img[k]= img[k]+\".jpg\"\n",
    "    print(img)\n",
    "    \n",
    "    while len(img)==1:\n",
    "        res = cv2.imread('D:/data/code/'+str(t)+'.jpg')\n",
    "        cv2.imshow(\"image\", res)\n",
    "        k = cv2.waitKey(1) & 0xFF\n",
    "        if k == 27:\n",
    "            break\n",
    "    cv2.destroyAllWindows()\n",
    "    if len(img)>1:\n",
    "        time.sleep(5)\n",
    "        clips = [ImageClip(m).set_duration(3)\n",
    "                  for m in img]\n",
    "        concat_clip = concatenate_videoclips(clips)\n",
    "        concat_clip.preview()\n",
    "    while True:\n",
    "        if keyboard.is_pressed('q'):  # if key 'q' is pressed \n",
    "            text_to_sign()\n",
    "            break\n",
    "        if keyboard.is_pressed('w'):  # if key 'w' is pressed \n",
    "            speech_to_sign()\n",
    "            break\n",
    "        if keyboard.is_pressed('e'):  # if key 'e' is pressed \n",
    "            sign_to_speech()\n",
    "            break\n",
    "        if keyboard.is_pressed('t'):  # if key 't' is pressed \n",
    "            text_to_speech()\n",
    "            break\n",
    "        if keyboard.is_pressed('c'):  # if key 'c' is pressed \n",
    "            colour_recognition()\n",
    "            break\n",
    "        if keyboard.is_pressed('r'):\n",
    "            print(\"program terminated\")# if key 'r' is pressed \n",
    "            break\n",
    "        \n",
    "def speech_to_sign():\n",
    "    r = sr.Recognizer()\n",
    "    mic = sr.Microphone()\n",
    "    with mic as audio_file:\n",
    "        print(\"Speak Please\")\n",
    "        r.adjust_for_ambient_noise(audio_file)\n",
    "        audio = r.listen(audio_file)\n",
    "        t= r.recognize_google(audio)\n",
    "        print(\"Converting Speech to Text...\")\n",
    "        print(\"You said: \" + t)\n",
    "    img=re.split('\\s+', t)\n",
    "    for k in range(len(img)):\n",
    "        img[k]= img[k]+\".jpg\"\n",
    "    print(img)\n",
    "    \n",
    "    while len(img)==1:\n",
    "        res = cv2.imread('D:/data/code/'+str(t)+'.jpg')\n",
    "        cv2.imshow(\"image\", res)\n",
    "        k = cv2.waitKey(1) & 0xFF\n",
    "        if k == 27:\n",
    "            break\n",
    "    if len(img)>1:\n",
    "        time.sleep(5)\n",
    "        clips = [ImageClip(m).set_duration(3)\n",
    "                  for m in img]\n",
    "        concat_clip = concatenate_videoclips(clips)\n",
    "        concat_clip.preview()\n",
    "    while True:\n",
    "        if keyboard.is_pressed('q'):  # if key 'q' is pressed \n",
    "            text_to_sign()\n",
    "            break\n",
    "        if keyboard.is_pressed('w'):  # if key 'w' is pressed \n",
    "            speech_to_sign()\n",
    "            break\n",
    "        if keyboard.is_pressed('e'):  # if key 'e' is pressed \n",
    "            sign_to_speech()\n",
    "            break\n",
    "        if keyboard.is_pressed('t'):  # if key 't' is pressed \n",
    "            text_to_speech()\n",
    "            break\n",
    "        if keyboard.is_pressed('c'):  # if key 'c' is pressed \n",
    "            colour_recognition()\n",
    "            break\n",
    "        if keyboard.is_pressed('r'):\n",
    "            print(\"program terminated\")# if key 'r' is pressed \n",
    "            break\n",
    "\n",
    "def text_to_speech():\n",
    "    msg=input(\"enter the message to speak: \")\n",
    "    say_text(msg)\n",
    "    while True:\n",
    "        if keyboard.is_pressed('q'):  # if key 'q' is pressed \n",
    "            text_to_sign()\n",
    "            break\n",
    "        if keyboard.is_pressed('w'):  # if key 'w' is pressed \n",
    "            speech_to_sign()\n",
    "            break\n",
    "        if keyboard.is_pressed('e'):  # if key 'e' is pressed \n",
    "            sign_to_speech()\n",
    "            break\n",
    "        if keyboard.is_pressed('t'):  # if key 't' is pressed \n",
    "            text_to_speech()\n",
    "            break\n",
    "        if keyboard.is_pressed('c'):  # if key 'c' is pressed \n",
    "            colour_recognition()\n",
    "            break\n",
    "        if keyboard.is_pressed('r'):\n",
    "            print(\"program terminated\")# if key 'r' is pressed \n",
    "            break\n",
    "\n",
    "def colour_recognition():\n",
    "    ROI_top = 100\n",
    "    ROI_bottom = 300\n",
    "    ROI_right = 150\n",
    "    ROI_left = 350\n",
    "    data=\"\"\n",
    "    text=\"\"\n",
    "    count_same_frame = 0\n",
    "    num_frames=0\n",
    "    \n",
    "    cap = cv2.VideoCapture(0)\n",
    "    (ret, frame) = cap.read()\n",
    "    prediction = 'n.a.'\n",
    "\n",
    "    PATH = './training.data'\n",
    "\n",
    "    if os.path.isfile(PATH) and os.access(PATH, os.R_OK):\n",
    "        print ('colour code')\n",
    "    else:\n",
    "        open('training.data', 'w')\n",
    "        color_histogram_feature_extraction.training()\n",
    "        print ('colour code')\n",
    "\n",
    "    while True:\n",
    "\n",
    "        ret, frame = cap.read()\n",
    "        roi = frame[ROI_top:ROI_bottom, ROI_right:ROI_left]\n",
    "        cv2.rectangle(frame, (ROI_left, ROI_top), (ROI_right, ROI_bottom), (255,128,0), 3)\n",
    "        histogram.color_histogram_of_test_image(roi)\n",
    "        prediction = knn.main('training.data', 'test.data')\n",
    "        old_text=text\n",
    "        cv2.putText(frame, str(count_same_frame), (30, 20), cv2.FONT_ITALIC, 0.5, (255,0,0), 1)\n",
    "        text = prediction\n",
    "        if prediction!= \"green\":\n",
    "            if old_text == text:\n",
    "                count_same_frame += 1\n",
    "            if old_text!=text:\n",
    "                count_same_frame=0\n",
    "            if count_same_frame == 30:\n",
    "                if text==\"blue\":\n",
    "                    text=\"additional troops about to join for support\"\n",
    "                if text==\"red\":\n",
    "                    text=\"ready to fight\"\n",
    "                if text==\"black\":\n",
    "                    text=\"mission aborted\"\n",
    "                if text==\"white\":\n",
    "                    text=\"ready to negotiate\"    \n",
    "                data=data+text\n",
    "                data+=\".\"\n",
    "                data+=\"\\n\"\n",
    "                count_same_frame=0\n",
    "        else:\n",
    "            if data != '':\n",
    "                Thread(target=say_text, args=(data, )).start()\n",
    "            text = \"\"\n",
    "            data = \"\"\n",
    "            count_same_frame=0\n",
    "                    \n",
    "        blackboard = np.zeros((480, 640, 3), dtype=np.uint8)\n",
    "        cv2.putText(blackboard, \" \", (180, 50), cv2.FONT_HERSHEY_TRIPLEX, 1.5, (255, 0,0))\n",
    "        cv2.putText(blackboard, \"Predicted text- \" + text, (30, 100), cv2.FONT_HERSHEY_TRIPLEX, 0.75, (255, 255, 0))\n",
    "        y0, dy = 140, 15\n",
    "        for i, data1 in enumerate(data.split('\\n')):\n",
    "            y = y0+i*dy\n",
    "            cv2.putText(blackboard, data1, (30, y), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255))\n",
    "        cv2.putText(frame,'Prediction: ' + prediction,(70, 45), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,255,255), 2)\n",
    "        cv2.rectangle(frame, (ROI_left, ROI_top), (ROI_right, ROI_bottom), (255,128,0), 3)\n",
    "        num_frames += 1\n",
    "        cv2.putText(frame, str(count_same_frame), (30, 20), cv2.FONT_ITALIC, 0.5, (0,255,255), 1)\n",
    "        res = np.hstack((frame, blackboard))\n",
    "        cv2.imshow(\"Recognizing gesture\", res)\n",
    "        k = cv2.waitKey(1) & 0xFF\n",
    "        if k == 27:\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    while True:\n",
    "        if keyboard.is_pressed('q'):  # if key 'q' is pressed \n",
    "            text_to_sign()\n",
    "            break\n",
    "        if keyboard.is_pressed('w'):  # if key 'w' is pressed \n",
    "            speech_to_sign()\n",
    "            break\n",
    "        if keyboard.is_pressed('e'):  # if key 'e' is pressed \n",
    "            sign_to_speech()\n",
    "            break\n",
    "        if keyboard.is_pressed('t'):  # if key 't' is pressed \n",
    "            text_to_speech()\n",
    "            break\n",
    "        if keyboard.is_pressed('c'):  # if key 'c' is pressed \n",
    "            colour_recognition()\n",
    "            break\n",
    "        if keyboard.is_pressed('r'):\n",
    "            print(\"program terminated\")# if key 'r' is pressed \n",
    "            break\n",
    "            \n",
    "while True:\n",
    "    if keyboard.is_pressed('q'):  # if key 'q' is pressed \n",
    "        text_to_sign()\n",
    "        break\n",
    "    if keyboard.is_pressed('w'):  # if key 'w' is pressed \n",
    "        speech_to_sign()\n",
    "        break\n",
    "    if keyboard.is_pressed('e'):  # if key 'e' is pressed \n",
    "        sign_to_speech()\n",
    "        break\n",
    "    if keyboard.is_pressed('t'):  # if key 't' is pressed \n",
    "        text_to_speech()\n",
    "        break\n",
    "    if keyboard.is_pressed('c'):  # if key 'r' is pressed \n",
    "        colour_recognition()\n",
    "        break\n",
    "    if keyboard.is_pressed('r'):\n",
    "        print(\"program terminated\")# if key 'r' is pressed \n",
    "        break\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "cr"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
